{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Engineering"
      ],
      "metadata": {
        "id": "vl6kd1EfoefS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "   - In feature engineering, a parameter typically refers to a configurable setting or value that influences how a feature is created, transformed, or selected.\n",
        "\n",
        "   - These parameters are not learned from the data (like model parameters), but rather are predefined  by the data scientist or engineer to control the behavior of preprocessing steps.\n",
        "\n",
        "   - The Key Difference:\n",
        "\n",
        "      1. Feature Engineering Parameters are set before training (part of preprocessing).\n",
        "      \n",
        "      2. Model Parameters (like weights in a neural network) are learned during training.\n",
        "\n",
        "   - Examples of Parameters in Feature Engineering:\n",
        "\n",
        "       1. Binning (Discretization) Parameters\n",
        "       \n",
        "          - Number of bins (n_bins). Bin edges or strategy (e.g., uniform, quantile).\n",
        "\n",
        "       2. Scaling/Normalization Parameters\n",
        "       \n",
        "          - With_mean, with_std (in standard Scaler) and Range for MinMaxScaler (e.g., feature_range=(0, 1)).\n",
        "\n",
        "       3. Imputation Parameters\n",
        "       \n",
        "          - Strategy for filling missing values (e.g., strategy='mean' in SimpleImputer).\n",
        "\n",
        "   - Types of Parameters in Feature Engineering:\n",
        "\n",
        "       1. Predefined Transformation Parameters -> Example: Choosing the window size for rolling averages in time-series data.\n",
        "\n",
        "       2. Learnable Model Parameters -> Example: Coefficients in a regression model.\n",
        "\n",
        "       3. Hyperparameters for Feature Processing -> Example: Deciding the degree of polynomial features for polynomial regression."
      ],
      "metadata": {
        "id": "sjpIkek2ovim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "   - Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "   \n",
        "   - In feature engineering, correlation refers to the statistical relationship between different features in a dataset.\n",
        "\n",
        "   - It helps identify patterns, feature selection, redundancy removal, and improving model efficiency.\n",
        "\n",
        "   - Range: Correlation values range from -1 to +1.\n",
        "\n",
        "      - +1 => Perfect positive correlation (as one variable increases, the other increases).\n",
        "      \n",
        "      - -1 => Perfect negative correlation (as one increases, the other decreases).\n",
        "      \n",
        "      - 0 => No linear relationship between the variables.\n",
        "\n",
        "   - Types of Correlation in Feature Engineering:\n",
        "\n",
        "     - Pearson Correlation (Linear relationship between continuous features).\n",
        "     \n",
        "     - Spearman Correlation (Monotonic relationship, useful for ordinal features).\n",
        "     \n",
        "     - Kendall’s Tau (Ranks the order of correlation).\n",
        "\n",
        "   - Why It Matters in Feature Engineering:\n",
        "\n",
        "      1. Detecting Redundant Features\n",
        "      \n",
        "         - If two features are highly correlated, one may be unnecessary and can be removed to avoid multicollinearity in models.\n",
        "\n",
        "      2. Selecting Relevant Features\n",
        "      \n",
        "         - Features that correlate well with the target variable are often valuable predictors in machine learning\n",
        "\n",
        "      3. Transforming Features\n",
        "      \n",
        "         - Identifying correlation patterns can guide feature transformations, such as combining correlated features into a single new feature.\n",
        "\n",
        "   - In feature engineering, a negative correlation means that as one feature increases, the other decreases.\n",
        "   \n",
        "   - This inverse relationship can be important when analyzing data for model performance and feature selection."
      ],
      "metadata": {
        "id": "YZFuVoB9sFej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "   - Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions or predictions with minimal human intervention.\n",
        "   \n",
        "   - Instead of being explicitly programmed to perform a task, ML algorithms improve automatically through experience.\n",
        "\n",
        "   -  In simple terms, Machine learning is teaching computers to learn from data and make decisions based on it.\n",
        "\n",
        "   - Main Components of Machine Learning:\n",
        "\n",
        "     1. Data:\n",
        "\n",
        "        - The foundation of any ML system. Data can be structured (like tables) or unstructured (like images or text).\n",
        "        \n",
        "        - Quality, quantity, and relevance of data heavily influence model performance.\n",
        "\n",
        "    2. Features:\n",
        "\n",
        "        - Individual measurable properties or attributes extracted from the data.\n",
        "\n",
        "        - Feature engineering refines these inputs to improve model accuracy.\n",
        "\n",
        "    3. Model:\n",
        "\n",
        "        - The algorithm or mathematical structure that learns from the data.\n",
        "          Examples: Linear regression, decision trees, neural networks.\n",
        "\n",
        "        - The model tries to find patterns or relationships between inputs (features) and outputs (labels).\n",
        "\n",
        "    4. Learning Algorithm:\n",
        "\n",
        "        - Defines how the model learns from data and adjusting parameters to improve performance.\n",
        "       \n",
        "        - Uses labeled datasets in supervised learning, and patterns in unsupervised learning.\n",
        "\n",
        "    5. Training Process:\n",
        "\n",
        "        - The model is fed with historical or labeled data to learn patterns.\n",
        "\n",
        "        - The process includes feeding inputs and known outputs to the algorithm so it can adjust its internal parameters.\n",
        "\n",
        "    6. Evaluation:\n",
        "\n",
        "       - Determines how well the model performs.\n",
        "\n",
        "       - Common metrics: accuracy, precision, recall, F1-score.\n",
        "\n",
        "    7. Inference & Deployment:\n",
        "\n",
        "       - Once trained, the model can make predictions on new, unseen data.\n",
        "\n",
        "       - Deploying in real-world applications (recommendation systems, fraud detection).\n",
        "\n",
        "    8. Feedback Loop (Optional):\n",
        "    \n",
        "       - Used in some systems (like recommendation engines or reinforcement learning) to continuously improve the model using new data or user feedback."
      ],
      "metadata": {
        "id": "hcgVIspuu46S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "   - The loss value is a crucial metric in machine learning that tells you how well or poorly your model is performing during training and evaluation.\n",
        "   \n",
        "   - It represents the difference between the model’s predictions and the actual target values.\n",
        "\n",
        "   - A lower loss generally indicates a better-performing model, while a high loss suggests poor accuracy or misaligned predictions.\n",
        "\n",
        "   - How Loss Value Helps Determine Model Quality:\n",
        "\n",
        "     1. Indicates Prediction Error:\n",
        "\n",
        "       - The loss function calculates how far off the model’s predicted outputs are from the true values.\n",
        "       \n",
        "       - A lower loss means your model is making predictions closer to the actual values and a higher loss means your model is making large errors.\n",
        "\n",
        "    2. Guides the Learning Process:\n",
        "    \n",
        "       - During training, the model uses a learning algorithm (e.g., gradient descent) to minimize the loss.\n",
        "       \n",
        "       - A good model is one where the loss steadily decreases over epochs (training cycles), ideally reaching a low, stable value.\n",
        "\n",
        "    3. Helps Detect Overfitting or Underfitting -> Compare training loss vs. validation loss:\n",
        "    \n",
        "       - Low training loss + high validation loss => Overfitting\n",
        "       \n",
        "       - (the model memorized training data but fails to generalize).\n",
        "       \n",
        "       - High training loss + high validation loss =>\n",
        "       \n",
        "       -  Underfitting (the model is too simple or not trained enough).Low and similar losses for both → Good generalization.\n",
        "\n",
        "    4. Comparing Model Variants:\n",
        "    \n",
        "       - Different architectures or hyperparameters can be compared using their loss values.\n",
        "       \n",
        "       - The model with the lowest loss while maintaining generalizability is preferable.\n",
        "\n",
        "  - Common Loss Functions:\n",
        "  \n",
        "     1. MSE (Mean Squared Error): Used in regression problems.\n",
        "     \n",
        "     2. Cross-Entropy Loss (Log Loss): Used in classification tasks.   "
      ],
      "metadata": {
        "id": "zDCeEgW1ytUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "   - Continuous and Categorical Variables are two fundamental types of data used in statistics and machine learning.\n",
        "   \n",
        "   - Understanding them helps you decide how to process, visualize, and model your data.\n",
        "\n",
        "   - In machine learning, variables are classified into continuous and categorical based on their characteristics.\n",
        "\n",
        "      1. Continuous Variables\n",
        "      \n",
        "         - A continuous variable is a numerical variable that can take any value within a range.\n",
        "         \n",
        "         - It is measurable and can have infinite possible values within an interval.\n",
        "\n",
        "         - Examples:\n",
        "         \n",
        "           1. Height (e.g., 170.5 cm)\n",
        "           \n",
        "           2. Weight (e.g., 65.3 kg)\n",
        "\n",
        "        - Characteristics:\n",
        "        \n",
        "           - Infinite or Fine-Grained Values: Can include decimals or fractions.\n",
        "           \n",
        "           - Mathematical Operations Valid: You can compute mean, variance, etc.\n",
        "           \n",
        "           - Visualized Using: Histograms, scatter plots, line graphs.\n",
        "\n",
        "        - Subtypes:\n",
        "        \n",
        "           1. Interval Variables:\n",
        "           \n",
        "              - No true zero (e.g., temperature in °C, where 0°C doesn’t mean \"no temperature\").\n",
        "              \n",
        "          2. Ratio Variables:\n",
        "          \n",
        "              - True zero exists (e.g., weight, height, income).\n",
        "\n",
        "      2. Categorical Variables\n",
        "\n",
        "         - A categorical variable represents qualitative data and takes on limited, fixed values that belong to distinct categories or groups.\n",
        "\n",
        "         - They can be nominal (no order) or ordinal (ordered categories).\n",
        "\n",
        "         - Examples:\n",
        "         \n",
        "            1. Country (e.g., USA, India, Brazil)\n",
        "            \n",
        "            2. Product category (e.g., Electronics, Clothing)\n",
        "\n",
        "        - Key Characteristics:\n",
        "        \n",
        "           - Limited Distinct Values: Fixed number of categories.\n",
        "           \n",
        "           - No Mathematical Meaning: Arithmetic operations (e.g., mean) are invalid.\n",
        "           \n",
        "           - Visualized Using: Bar charts, pie charts, frequency tables.\n",
        "\n",
        "        - Subtypes:\n",
        "        \n",
        "           1. Nominal Variables:\n",
        "           \n",
        "              - No order or ranking (e.g., colors, countries).\n",
        "              \n",
        "              - Example: [\"Dog\", \"Cat\", \"Bird\"] (no inherent ranking).\n",
        "              \n",
        "          2. Ordinal Variables:\n",
        "          \n",
        "             - Categories have a meaningful order but intervals are not uniform.\n",
        "             \n",
        "             - Example: [\"Low\", \"Medium\", \"High\"] Likert scales (1 = Strongly Disagree, 5 = Strongly Agree)."
      ],
      "metadata": {
        "id": "QLOzkheq_-lM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "   - In Machine Learning, categorical variables must be converted into a numerical format so that models can process them effectively.\n",
        "   \n",
        "   - There are several techniques for handling categorical variables depending on the type of data and the model being used.\n",
        "\n",
        "   - Common Techniques to Handle Categorical Variables:\n",
        "\n",
        "       1. Label Encoding:\n",
        "\n",
        "           - Assigns numerical labels to each unique category.\n",
        "           \n",
        "           - Works well for ordinal variables (where categories have a meaningful order).\n",
        "\n",
        "       2. One-Hot Encoding:\n",
        "       \n",
        "           - Creates binary columns for each category (1 if present, 0 if not).\n",
        "           \n",
        "           - Works well for nominal variables (no meaningful order).\n",
        "\n",
        "       3. Binary Encoding:\n",
        "       \n",
        "          - Converts categories into binary values and encodes them in fewer columns than One-Hot Encoding.\n",
        "          \n",
        "          - Useful for datasets with high cardinality (many unique values).\n",
        "\n",
        "       4. Frequency Encoding:\n",
        "       \n",
        "          - Replaces categories with the frequency of their occurrence in the dataset.\n",
        "          \n",
        "          - Helps in models where category frequency is relevant (e.g., fraud detection).\n",
        "\n",
        "       5. Target Encoding (Mean Encoding):\n",
        "       \n",
        "          - Maps each category to the average target variable value (works well in supervised learning).\n",
        "          \n",
        "          - Useful for categorical variables with a strong relationship to the dependent variable.\n",
        "\n",
        "       6. Embedding Layers (For Deep Learning):\n",
        "      \n",
        "          - Assigns dense vector representations to categories, making them more informative for neural networks.\n",
        "          \n",
        "          - Used in Natural Language Processing (NLP) and recommender systems."
      ],
      "metadata": {
        "id": "BAYd_tzzDRMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "   - In Machine Learning, training and testing a dataset refers to the process of splitting data to build and evaluate a model.\n",
        "\n",
        "   - Training and testing datasets are used to build and evaluate models effectively.\n",
        "   \n",
        "   - They help ensure that the model can learn patterns and generalize well to new, unseen data.\n",
        "\n",
        "     1. Training dataset:\n",
        "     \n",
        "         - The training dataset is the portion of data used to train the machine learning model.\n",
        "         \n",
        "         - The model learns patterns, relationships, and structures from this data by adjusting its parameters to minimize errors (loss).\n",
        "\n",
        "         - The model adjusts parameters based on this data.\n",
        "\n",
        "         - Example: If building a fraud detection model, the training dataset contains past transactions labeled as \"fraud\" or \"not fraud.\"\n",
        "\n",
        "    2. Testing dataset:\n",
        "\n",
        "        - The testing dataset is separate from the training data and is used to evaluate how well the trained model performs on unseen data.\n",
        "        \n",
        "        - It helps estimate the model’s real-world performance and checks if it’s overfitting or underfitting.\n",
        "\n",
        "        - Helps determine how well the model generalizes beyond the training data.\n",
        "        \n",
        "        - Example: If training a model to predict house prices, the test dataset contains homes it has never seen before.\n",
        "\n",
        "   - Key points:\n",
        "\n",
        "      - The testing dataset should never be used during training.\n",
        "\n",
        "      - Typical split ratio are 80% training / 20% testing Or 70% / 30%. Sometimes a validation set is also used for tuning parameters (common split: 60/20/20)."
      ],
      "metadata": {
        "id": "Dpb8y0LpFBa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "   - sklearn.preprocessing is a module in Scikit-Learn that provides various methods for transforming and scaling data before feeding it into a machine learning model.\n",
        "   \n",
        "   - It ensures features are well-conditioned for training, improving model accuracy and convergence.\n",
        "\n",
        "   - Key Functions in sklearn.preprocessing:\n",
        "\n",
        "      1. Standardization (Scaling Data):\n",
        "      \n",
        "         - Ensures features have zero mean and unit variance (useful for models like SVM, logistic regression).\n",
        "\n",
        "      2. Min-Max Scaling (Normalization):\n",
        "      \n",
        "         - Rescales data into a fixed range (e.g., 0 to 1), useful for neural networks.\n",
        "\n",
        "      3. Label Encoding & One-Hot Encoding:\n",
        "      \n",
        "         - Converts categorical labels into numerical form.\n",
        "\n",
        "      4. Polynomial Features:\n",
        "      \n",
        "         - Generates polynomial terms for feature expansion, improving non-linear relationships.\n",
        "\n",
        "      5. Binarization:\n",
        "      \n",
        "         - Converts values into binary format based on a threshold.\n",
        "\n",
        "   - Why Preprocessing Matters\n",
        "   \n",
        "     -  Most machine learning algorithms expect numerical, scaled, and clean input. Preprocessing helps by:\n",
        "   \n",
        "      1. Handling missing values\n",
        "      \n",
        "      2. Converting categorical data to numbers\n",
        "      \n",
        "      3. Scaling/normalizing data\n",
        "      \n",
        "      4. Encoding features"
      ],
      "metadata": {
        "id": "Z909qiCLGzDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "   - A test set is a portion of a dataset used to evaluate the performance of a trained machine learning model.\n",
        "   \n",
        "   - Unlike the training set, which is used to teach the model patterns, the test set consists of unseen data that helps determine how well the model generalizes beyond what it has learned.\n",
        "\n",
        "   - Why Use a Test Set?\n",
        "   \n",
        "      1. Assess Model Accuracy\n",
        "      \n",
        "         - Helps measure how well the model makes predictions on new data.\n",
        "      \n",
        "      2. Avoid Overfitting\n",
        "      \n",
        "         -  Ensures the model is not just memorizing the training data but actually learning meaningful patterns.\n",
        "      \n",
        "      3. Compare Different Models\n",
        "      \n",
        "         -  Used to benchmark multiple models and select the best-performing one.\n",
        "\n",
        "  - Typical Data Splitting:\n",
        "    \n",
        "      1.  Training Set\n",
        "       \n",
        "          - To train the model (learn patterns)\n",
        "          \n",
        "      2. Validation Set\n",
        "       \n",
        "          - To tune model hyperparameters\n",
        "          \n",
        "      3. Test Set\n",
        "       \n",
        "          - To evaluate final model accuracy\n",
        "\n",
        "  - A common split is:\n",
        "    \n",
        "      - 60% training\n",
        "       \n",
        "      - 20% validation\n",
        "       \n",
        "      - 20% testing\n",
        "       \n",
        "      - Or if no validation set is used:\n",
        "      \n",
        "      - 80% training\n",
        "      \n",
        "      - 20% testing\n",
        "\n",
        "  - Important Notes:\n",
        "  \n",
        "     - Never train on the test set — using it during training leads to biased results.\n",
        "     \n",
        "     - Use the test set only once, after tuning and selecting your final model."
      ],
      "metadata": {
        "id": "KqwU6FTXMRGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "   1. How to Split Data for Model Fitting (Training and Testing) in Python\n",
        "   \n",
        "      - In machine learning, splitting data into training and testing sets is essential to evaluate model performance.\n",
        "   \n",
        "      -  The standard approach is using Scikit-Learn's train_test_split function, which ensures an efficient data split.\n",
        "\n",
        "      - What this does:\n",
        "   \n",
        "         - test_size=0.2 → 20% of data goes to the test set.\n",
        "      \n",
        "         - random_state=42 → Ensures reproducibility of splits.\n",
        "      \n",
        "         -  X_train, X_test → Features for training & testing.\n",
        "      \n",
        "        -  y_train, y_test → Labels for model evaluation.\n",
        "\n",
        "   2. Approaching a Machine Learning Problem\n",
        "   \n",
        "      - A structured approach improves efficiency and ensures a well-performing model.\n",
        "      \n",
        "      - Here's a practical step-by-step process:\n",
        "\n",
        "        1. Understanding the Problem\n",
        "        \n",
        "            - Define the objective (classification, regression, clustering).\n",
        "            \n",
        "            - Identify key variables (features & target labels) and understand business context & impact.\n",
        "\n",
        "        2. Collect & Prepare Data\n",
        "        \n",
        "            - Acquire relevant datasets (structured, unstructured).\n",
        "            \n",
        "            - Handle missing values (imputation, removal) and perform exploratory data analysis (EDA) to detect patterns.\n",
        "\n",
        "        3. Feature Engineering & Selection\n",
        "       \n",
        "            - Identify meaningful features (domain knowledge helps!).\n",
        "           \n",
        "            - Apply transformations (scaling, normalization, encoding) and reduce dimensionality if needed (PCA, feature selection).\n",
        "\n",
        "        4. Choose & Train the Model\n",
        "        \n",
        "            - Select a model (Linear Regression, Random Forest, Neural Networks).\n",
        "            \n",
        "            - Split data into training and testing sets.\n",
        "            \n",
        "            - Train the model and tune hyperparameters.\n",
        "\n",
        "        5. . Evaluate Model Performance\n",
        "        \n",
        "            - Measure accuracy using relevant metrics (RMSE, Precision, Recall, F1-score).\n",
        "            \n",
        "            - Use cross-validation to improve generalization.\n",
        "            \n",
        "            - Compare results across different models.\n",
        "\n",
        "        6. Deployment & Continuous Improvement\n",
        "        \n",
        "            - Deploy the model into production.\n",
        "            \n",
        "            - Monitor real-world predictions & feedback.\n",
        "            \n",
        "            - Continuously retrain using updated datasets."
      ],
      "metadata": {
        "id": "5jO0u_GfOTrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for question 10 Split data for model fitting in python.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\", X_train)\n",
        "print(\"Test Set:\", X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziALmkjQO6Ho",
        "outputId": "ea5af5ed-483c-4c8d-a028-897008e31a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: [[ 6]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [10]\n",
            " [ 5]\n",
            " [ 4]\n",
            " [ 7]]\n",
            "Test Set: [[9]\n",
            " [2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    \n",
        "    - Exploratory Data Analysis (EDA) is a crucial first step in any machine learning project. It helps you understand your data deeply before building a model.\n",
        "\n",
        "    - Skipping EDA can lead to poorly trained models, biased predictions, and unexpected errors.\n",
        "\n",
        "    - Why EDA is Essential Before Model Fitting:\n",
        "\n",
        "       1. Understand the Structure of the Data:\n",
        "       \n",
        "           - Know the types of variables (numerical, categorical, date/time)\n",
        "           \n",
        "           - Identify the shape of the dataset (rows × columns)\n",
        "           \n",
        "           - Spot high-level patterns and distributions\n",
        "\n",
        "       2. Identify and Handle Missing Values:\n",
        "       \n",
        "           - EDA helps detect missing or null values.\n",
        "           \n",
        "           - After detection one can decide how to handle them: remove, fill (mean, median), or flag.\n",
        "\n",
        "           - Example: If income data has extreme values, they might distort regression results.\n",
        "\n",
        "       3. Detect Outliers or Anomalies:\n",
        "\n",
        "           - Boxplots, histograms, and scatter plots reveal outliers\n",
        "           \n",
        "           - Outliers can skew your model and reduce accuracy\n",
        "\n",
        "           - Example: A house price of $1,000,000 in a dataset of $100k houses.\n",
        "\n",
        "      4. Understand Feature Distributions:\n",
        "        \n",
        "           - Helps visualize if features follow a normal, skewed, or uniform distribution.\n",
        "           \n",
        "           - Helps decide whether to scale, log-transform, or bin values.\n",
        "\n",
        "           - Example: Some ML models assume normally distributed data—EDA confirms if transformations (like log scaling) are needed.\n",
        "\n",
        "      5. Assessing Correlations Between Features:\n",
        "\n",
        "           - Identifies relationships between variables to remove redundancy.\n",
        "\n",
        "           - Use correlation heatmaps, pair plots, and group-by summaries.\n",
        "\n",
        "           - Example: If two features are highly correlated (like height & weight), one might be dropped.\n",
        "\n",
        "      6. Detect Data Leakage or Bias:\n",
        "      \n",
        "           - Identify if certain variables \"leak\" future information (e.g., using 'final_result' to predict 'final_result')\n",
        "           \n",
        "           - Look for class imbalance in classification problems.\n",
        "\n",
        "           - Example: 95% of data is \"No Churn\", so accuracy is misleading.\n",
        "\n",
        "      7. Choosing the Right Preprocessing Steps:\n",
        "      \n",
        "         - Determines whether scaling, normalization, or encoding is needed.\n",
        "         \n",
        "         - Example: Categorical variables must be encoded before being used in ML models.\n",
        "\n",
        "     8. Validating Assumptions About Data:\n",
        "     \n",
        "         - Confirms if the data aligns with expectations for specific models.\n",
        "         \n",
        "         - Example: Linear regression assumes linear relationships—EDA ensures this holds."
      ],
      "metadata": {
        "id": "sLdFw3VHWWe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "    -  - Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "   \n",
        "   - In feature engineering, correlation refers to the statistical relationship between different features in a dataset.\n",
        "\n",
        "   - It helps identify patterns, feature selection, redundancy removal, and improving model efficiency.\n",
        "\n",
        "   - Range: Correlation values range from -1 to +1.\n",
        "\n",
        "      - +1 => Perfect positive correlation (as one variable increases, the other increases).\n",
        "      \n",
        "      - -1 => Perfect negative correlation (as one increases, the other decreases).\n",
        "      \n",
        "      - 0 => No linear relationship between the variables.\n",
        "\n",
        "   - Types of Correlation in Feature Engineering:\n",
        "\n",
        "     - Pearson Correlation (Linear relationship between continuous features).\n",
        "     \n",
        "     - Spearman Correlation (Monotonic relationship, useful for ordinal features).\n",
        "     \n",
        "     - Kendall’s Tau (Ranks the order of correlation).\n",
        "\n",
        "   - Why It Matters in Feature Engineering:\n",
        "\n",
        "      1. Detecting Redundant Features\n",
        "      \n",
        "         - If two features are highly correlated, one may be unnecessary and can be removed to avoid multicollinearity in models.\n",
        "\n",
        "      2. Selecting Relevant Features\n",
        "      \n",
        "         - Features that correlate well with the target variable are often valuable predictors in machine learning\n",
        "\n",
        "      3. Transforming Features\n",
        "      \n",
        "         - Identifying correlation patterns can guide feature transformations, such as combining correlated features into a single new feature."
      ],
      "metadata": {
        "id": "f5jXeLpzZzZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "   \n",
        "    - Negative correlation means that as one variable increases, the other decreases. It's an inverse relationship between two variables.\n",
        "    \n",
        "    - This inverse relationship is important in statistics, data analysis, and machine learning when assessing how variables interact.\n",
        "\n",
        "    - Negative correlation, usually between −0.1 and −1.0 depending on the strength.\n",
        "\n",
        "    - Example in Real Life:\n",
        "    \n",
        "       - The more hours a student studies (↑), the fewer errors they make in exams (↓).\n",
        "\n",
        "       - Exercise vs. Body Fat → More exercise is typically associated with lower body fat."
      ],
      "metadata": {
        "id": "gf4O4vCOaMEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "   - The correlation between variables can be find in Python using libraries like NumPy and Pandas.\n",
        "   \n",
        "   - The correlation coefficient quantifies the strength and direction of the relationship between two variables.\n",
        "\n",
        "   - Methods to Find Correlation:\n",
        "\n",
        "      1. Using NumPy (corrcoef)\n",
        "      \n",
        "         - Computes Pearson correlation between two numerical arrays.\n",
        "\n",
        "      2. Using Pandas (corr)\n",
        "      \n",
        "         - Generates a correlation matrix for all numerical columns in a DataFrame."
      ],
      "metadata": {
        "id": "UT0IMxjMa-Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding correlation using NumPy (corrcoef)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = [10, 20, 30, 40, 50]\n",
        "y = [5, 10, 15, 20, 25]\n",
        "\n",
        "correlation = np.corrcoef(x, y)[0, 1]\n",
        "print(\"Correlation:\", correlation)  # Output: 1.0 (perfect positive correlation)\n",
        "\n",
        "# Finding correlation using Pandas (corr)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Age': [20, 25, 30, 35, 40], 'Salary': [2000, 3000, 4000, 5000, 6000]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQkn-GYEbq3K",
        "outputId": "4edb2fc9-f1aa-475c-87ba-08f240ca9879"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation: 1.0\n",
            "        Age  Salary\n",
            "Age     1.0     1.0\n",
            "Salary  1.0     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "    - Causation means that one variable directly causes a change in another variable. If A causes B, then changing A will result in a change in B.\n",
        "    \n",
        "    - This is also known as a cause-and-effect relationship.\n",
        "\n",
        "    - Difference between correlation and causation:\n",
        "\n",
        "       1. Correlation\n",
        "\n",
        "           - Two variables move together (but one does not necessarily cause the other).\n",
        "\n",
        "           - Measures statistical association (X and Y vary together).\n",
        "\n",
        "           - No directionality\n",
        "\n",
        "           - Can be coincidental (spurious).\n",
        "\n",
        "           - Example: Ice cream sales ~ drowning. These two are positively correlated.But eating ice cream doesn't cause drowning.The real cause is a third factor: hot weather increases both.\n",
        "\n",
        "       2. Causation:\n",
        "\n",
        "           - One variable directly influences the other.\n",
        "\n",
        "           - Implies X directly changes Y.\n",
        "\n",
        "           - Directional (X → Y).\n",
        "\n",
        "           - Requires evidence beyond data.\n",
        "\n",
        "           - Example: Study time vs. exam results. Here, increasing hours studied leads to higher exam scores.There's a direct cause-effect relationship."
      ],
      "metadata": {
        "id": "0JYH71oHcRT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "   - An optimizer is an algorithm that helps a machine learning model adjust its parameters to minimize errors and improve performance. It updates model weights based on the loss function to ensure better predictions.\n",
        "   \n",
        "   - It determines how the model learns from data by updating parameters iteratively during training.\n",
        "\n",
        "   - The goal of training a model is to find the best parameters (weights) that result in minimal prediction error.Optimizers decide how fast and in what direction the model learns.\n",
        "\n",
        "   - Common Optimizers in Machine Learning:\n",
        "\n",
        "      1. SGD (Stochastic Gradient Descent)\n",
        "\n",
        "          - Simple, updates weights after each training example. May be noisy, but can help escape local minima.\n",
        "\n",
        "          - Best use case small datasets, simple models\n",
        "\n",
        "      2. Momentum\n",
        "\n",
        "         - Speeds up SGD using past updates.\n",
        "\n",
        "         - Best use case faster convergence.\n",
        "\n",
        "     3. RMSprop (Root Mean Square Propagation)\n",
        "     \n",
        "        - Adapts learning rate per parameter\n",
        "\n",
        "        - Best use case RNNs, sequence models\n",
        "\n",
        "     4. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "        - Combines Momentum and RMSprop\n",
        "\n",
        "        - Very efficient and works well with most deep learning tasks.\n",
        "\n",
        "     5. Adagrad\n",
        "     \n",
        "        - Increases learning rate for rare features and decreases it for frequent ones.Adapts learning rate per feature\n",
        "        \n",
        "        - Best use case Sparse data, NLP.\n",
        "\n",
        "     6. Adadelta\n",
        "    \n",
        "       - Addresses limitations of Adagrad by using a moving window of gradients. Improvement on Adagrad\n",
        "       \n",
        "       - Robust on noisy data    "
      ],
      "metadata": {
        "id": "nNa97tfVe_Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model?\n",
        "\n",
        "    - sklearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks.\n",
        "\n",
        "    - It includes algorithms based on linear relationships between variables and is widely used for predictive modeling.\n",
        "\n",
        "    - When to Use sklearn.linear_model?\n",
        "\n",
        "       - Regression Problems (Continuous predictions → Linear, Ridge, Lasso)\n",
        "       \n",
        "       - Classification Problems (Categorical predictions → Logistic, Perceptron, SGDClassifier)\n",
        "       \n",
        "       - High-Dimensional Data (Regularization needed → Ridge, Lasso)\n",
        "\n",
        "    -  Key Algorithms in sklearn.linear_model:\n",
        "\n",
        "        1. LinearRegression()\n",
        "        \n",
        "           - Used case Regression\n",
        "           \n",
        "           - Ordinary Least Squares Linear Regression\n",
        "\n",
        "        2. LogisticRegression()\n",
        "        \n",
        "           - Used case Classification\n",
        "           \n",
        "           - Logistic Regression (for binary/multi-class classification)\n",
        "\n",
        "        3. Ridge()\n",
        "        \n",
        "          - Used case Regression\n",
        "          \n",
        "          - Linear regression + L2 regularization\n",
        "          \n",
        "        4. Lasso()\n",
        "        \n",
        "          - Used case Regression\n",
        "          \n",
        "          - Linear regression + L1 regularization\n",
        "          \n",
        "        5. ElasticNet()\n",
        "        \n",
        "          - Used case Regression\n",
        "          \n",
        "          - Combines Lasso and Ridge\n",
        "          \n",
        "        6. SGDClassifier()\n",
        "        \n",
        "          - Used case Classification\n",
        "          \n",
        "          - Linear classifiers with SGD\n",
        "          \n",
        "        7. SGDRegressor()\n",
        "        \n",
        "         - Used case Regression\n",
        "         \n",
        "         - Linear regression with SGD\n",
        "         \n",
        "        8. Perceptron()\n",
        "        \n",
        "         - Used case Binary classification\n",
        "         \n",
        "         - A simple linear binary classifier\n",
        "         \n",
        "        9. BayesianRidge()\n",
        "        \n",
        "          - Used case Regression\n",
        "          \n",
        "          - Bayesian linear regression\n",
        "          \n",
        "        10. PassiveAggressiveClassifier()\n",
        "        \n",
        "         - Used case online classification\n",
        "        \n",
        "         - For large-scale, real-time classification"
      ],
      "metadata": {
        "id": "l7mTyI7sh3KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "   - model.fit() is a key function in Scikit-Learn, TensorFlow, and PyTorch that trains a machine learning model using the provided data.\n",
        "   \n",
        "   - It adjusts model parameters based on training data to minimize errors and improve predictions.\n",
        "\n",
        "   - How model.fit() Works:\n",
        "\n",
        "     1. Takes Input Data (X_train) and Labels (y_train)\n",
        "     \n",
        "       - X_train contains features (inputs), and y_train contains target values (labels).\n",
        "\n",
        "     2. Applies Optimization (Gradient Descent)\n",
        "     \n",
        "       - Updates model weights to minimize the loss function.\n",
        "\n",
        "     3. Learns Patterns from Data\n",
        "     \n",
        "       - The model iterates over the dataset, adjusting parameters to improve accuracy.\n",
        "\n",
        "  - Common Arguments in model.fit()\n",
        "\n",
        "     1. X_train\n",
        "     \n",
        "       - Input features (numeric data, images, text, etc.)\n",
        "       \n",
        "       - Example: X_train = [[1], [2], [3]]\n",
        "       \n",
        "     2. y_train\n",
        "     \n",
        "       - Target labels (dependent variable)\n",
        "       \n",
        "       - Example: y_train = [10, 20, 30]\n",
        "    \n",
        "     3. epochs\n",
        "     \n",
        "       - Number of times the model sees the data (for deep learning)\n",
        "       \n",
        "       - Example: epochs=100\n",
        "       \n",
        "     4. batch_size\n",
        "     \n",
        "       - Number of samples per training step (for deep learning)\n",
        "       \n",
        "       - Example: batch_size=32\n",
        "       \n",
        "     5. verbose\n",
        "     \n",
        "       - Controls output display during training\n",
        "       \n",
        "       - Example: verbose=1"
      ],
      "metadata": {
        "id": "eBGRbsMiXNRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "   - model.predict() is used in Scikit-Learn and other machine learning frameworks to generate predictions based on a trained model.\n",
        "   \n",
        "   - Once the model has learned patterns from training data using model.fit(), model.predict() applies those learned patterns to new, unseen data.\n",
        "\n",
        "   - How It Works\n",
        "   \n",
        "     - The function takes new input features and returns the predicted output based on the model’s learned parameters.\n",
        "     \n",
        "     - It works for both regression (predicting continuous values (e.g., 45.2, 88.6)) and classification (predicting categories (e.g., 0, 1, or 'Yes', 'No'))\n",
        "\n",
        "   - Arguments for model.predict()\n",
        "   \n",
        "      1. X_new\n",
        "      \n",
        "        - New input data/features to make predictions\n",
        "\n",
        "        - This must match the shape and format of the training data used in .fit().\n",
        "        \n",
        "        - It does not require the y (target/output), since we’re predicting it.\n",
        "        \n",
        "        - Example: X_new = [[6], [7]]  "
      ],
      "metadata": {
        "id": "-cpGRFlMZISy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "   -  Continuous and Categorical Variables are two fundamental types of data used in statistics and machine learning.\n",
        "   \n",
        "   - Understanding them helps you decide how to process, visualize, and model your data.\n",
        "\n",
        "   - In machine learning, variables are classified into continuous and categorical based on their characteristics.\n",
        "\n",
        "      1. Continuous Variables\n",
        "      \n",
        "         - A continuous variable is a numerical variable that can take any value within a range.\n",
        "         \n",
        "         - It is measurable and can have infinite possible values within an interval.\n",
        "\n",
        "         - Examples:\n",
        "         \n",
        "           1. Height (e.g., 170.5 cm)\n",
        "           \n",
        "           2. Weight (e.g., 65.3 kg)\n",
        "\n",
        "        - Characteristics:\n",
        "        \n",
        "           - Infinite or Fine-Grained Values: Can include decimals or fractions.\n",
        "           \n",
        "           - Mathematical Operations Valid: You can compute mean, variance, etc.\n",
        "           \n",
        "           - Visualized Using: Histograms, scatter plots, line graphs.\n",
        "\n",
        "        - Subtypes:\n",
        "        \n",
        "           1. Interval Variables:\n",
        "           \n",
        "              - No true zero (e.g., temperature in °C, where 0°C doesn’t mean \"no temperature\").\n",
        "              \n",
        "          2. Ratio Variables:\n",
        "          \n",
        "              - True zero exists (e.g., weight, height, income).\n",
        "\n",
        "      2. Categorical Variables\n",
        "\n",
        "         - A categorical variable represents qualitative data and takes on limited, fixed values that belong to distinct categories or groups.\n",
        "\n",
        "         - They can be nominal (no order) or ordinal (ordered categories).\n",
        "\n",
        "         - Examples:\n",
        "         \n",
        "            1. Country (e.g., USA, India, Brazil)\n",
        "            \n",
        "            2. Product category (e.g., Electronics, Clothing)\n",
        "\n",
        "        - Key Characteristics:\n",
        "        \n",
        "           - Limited Distinct Values: Fixed number of categories.\n",
        "           \n",
        "           - No Mathematical Meaning: Arithmetic operations (e.g., mean) are invalid.\n",
        "           \n",
        "           - Visualized Using: Bar charts, pie charts, frequency tables.\n",
        "\n",
        "        - Subtypes:\n",
        "        \n",
        "           1. Nominal Variables:\n",
        "           \n",
        "              - No order or ranking (e.g., colors, countries).\n",
        "              \n",
        "              - Example: [\"Dog\", \"Cat\", \"Bird\"] (no inherent ranking).\n",
        "              \n",
        "          2. Ordinal Variables:\n",
        "          \n",
        "             - Categories have a meaningful order but intervals are not uniform.\n",
        "             \n",
        "             - Example: [\"Low\", \"Medium\", \"High\"] Likert scales (1 = Strongly Disagree, 5 = Strongly Agree)."
      ],
      "metadata": {
        "id": "md25UukcaZu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "   - Feature scaling is a data preprocessing technique used to normalize or standardize the range of independent features (input variables) in a dataset.\n",
        "   \n",
        "   - Different features may have different units or scales (e.g., age in years vs. income in thousands), and this can negatively affect model performance.\n",
        "\n",
        "   - Importance of feature scaling in Machine Learning:\n",
        "\n",
        "     1. Improves Model Convergence\n",
        "\n",
        "       - Many optimization algorithms (like Gradient Descent) perform better when features are on similar scales.\n",
        "       \n",
        "       - Without scaling, models may struggle to converge or take longer to train.\n",
        "\n",
        "     2. Prevents Bias in Distance-Based Models\n",
        "\n",
        "       - lgorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Principal Component Analysis (PCA) are sensitive to large feature values.\n",
        "       \n",
        "       - Example: If a dataset has height (in meters) and salary (in dollars), the salary values dominate calculations.\n",
        "\n",
        "     3. Enhances Model Performance\n",
        "     \n",
        "       - Ensures that all features contribute equally to predictions.\n",
        "       \n",
        "       - Particularly important in deep learning, where activation functions perform better with scaled inputs.\n",
        "\n",
        "   - Algorithms That Are Sensitive to Feature Scale:\n",
        "\n",
        "     1. K-Nearest Neighbors (KNN)\n",
        "     \n",
        "       - Uses distance metrics (e.g., Euclidean)\n",
        "       \n",
        "     2. Support Vector Machines (SVM)\n",
        "    \n",
        "       - Uses dot products and distances\n",
        "      \n",
        "     3. Logistic/Linear Regression\n",
        "    \n",
        "       - Gradient descent-based optimization\n",
        "      \n",
        "     4. Neural Networks\n",
        "    \n",
        "       - Faster training and convergence\n",
        "     \n",
        "     5. PCA (Principal Component Analysis)\n",
        "    \n",
        "       -\tVariance-based dimensionality reduction\n",
        "\n",
        "   - Common Feature Scaling Techniques:\n",
        "   \n",
        "     1. Min-Max Scaling\n",
        "     \n",
        "       - Scales data to a fixed range [0, 1]\n",
        "\n",
        "       - MinMaxScaler()    \n",
        "     \n",
        "     2. Standardization (Z-score)\n",
        "     \n",
        "       - Mean = 0, Standard Deviation = 1\n",
        "       \n",
        "       - StandardScaler()\n",
        "       \n",
        "     3. Robust Scaling\n",
        "     \n",
        "       - Uses median & IQR, useful with outliers\n",
        "       \n",
        "       - RobustScaler()\n",
        "       \n",
        "     4. MaxAbs Scaling\n",
        "    \n",
        "       - Scales features by maximum absolute value\n",
        "       \n",
        "       - MaxAbsScaler()"
      ],
      "metadata": {
        "id": "aZ0OPsvZa9k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "   - In Python, scaling typically refers to feature scaling, which is the process of normalizing or standardizing data to bring all features onto a similar scale.\n",
        "   \n",
        "   - This is especially important for machine learning algorithms like KNN, SVM, and gradient descent-based models.\n",
        "   \n",
        "   - Here's how to perform scaling using Python, particularly with the scikit-learn library.\n",
        "\n",
        "       1.  Using StandardScaler (Standardization / Z-score Normalization)\n",
        "       \n",
        "          - This scales data to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "       2. Using MinMaxScaler (Normalization)    \n",
        "       \n",
        "         - This scales features to a fixed range, usually [0, 1].\n",
        "\n",
        "       3.  Using RobustScaler\n",
        "       \n",
        "         - Useful when data contains outliers. It scales using the median and interquartile range."
      ],
      "metadata": {
        "id": "Bb0UdLGndjpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Using StandardScaler (Standardization / Z-score Normalization)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzBaA0JTeJo4",
        "outputId": "953424a8-1d5a-4e12-bf70-162348ec079a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using MinMaxScaler (Normalization)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJBoEqXecOF",
        "outputId": "45430179-b254-48d5-ee6e-5032bc1fb075"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Using RobustScaler\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [100, 200]]  # outlier present\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bMorvgMene0",
        "outputId": "5ee3a9c5-9234-4755-cd4e-c0592d7fcf10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.04040404 -0.02020202]\n",
            " [ 0.          0.        ]\n",
            " [ 1.95959596  1.97979798]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "   - sklearn.preprocessing is a module in Scikit-Learn that provides various methods for transforming and scaling data before feeding it into a machine learning model.\n",
        "   \n",
        "   - It ensures that features are well-conditioned, improving model accuracy and convergence.\n",
        "\n",
        "   - Key Functions in sklearn.preprocessing:\n",
        "\n",
        "      1. Standardization (Scaling Data)\n",
        "      \n",
        "        - Ensures features have zero mean and unit variance (useful for models like SVM and logistic regression).\n",
        "\n",
        "      2. Min-Max Scaling (Normalization)\n",
        "      \n",
        "        - Rescales data into a fixed range (e.g., 0 to 1), useful for neural networks.\n",
        "        \n",
        "      3. Label Encoding & One-Hot Encoding\n",
        "      \n",
        "        - Converts categorical labels into numerical form.\n",
        "\n",
        "     4. Polynomial Features\n",
        "     \n",
        "       - Generates polynomial terms for feature expansion, improving non-linear relationships.\n",
        "\n",
        "     5. Binarization\n",
        "     \n",
        "       - Converts values into binary format based on a threshold.\n",
        "\n",
        "  - Why Use sklearn.preprocessing?\n",
        "  \n",
        "     1. Improves Model Convergence (Normalization & Standardization).\n",
        "     \n",
        "     2. Enhances Feature Representation (Polynomial Features).\n",
        "    \n",
        "     3. Handles Categorical Data (Label Encoding, One-Hot Encoding).\n",
        "    \n",
        "     4. Prepares Data for Robust Models (Scaling & Transformation)."
      ],
      "metadata": {
        "id": "pzVIEI8Hetk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "    - In machine learning, it's essential to split data into training and testing sets to evaluate model performance.\n",
        "    \n",
        "    - The most common approach is using Scikit-Learn's train_test_split function, which efficiently divides data.\n",
        "\n",
        "    - Remember always split your data before scaling or preprocessing to avoid data leakage.\n",
        "\n",
        "    - Why Split Data?\n",
        "    \n",
        "       - Training Set: Used to teach the model patterns in the data.\n",
        "       \n",
        "       - Testing Set: Used to assess the model’s ability to generalize to unseen data.\n",
        "\n",
        "    - Step-by-Step Guide:\n",
        "\n",
        "       1. Import the function\n",
        "\n",
        "       2. Prepare your features and labels\n",
        "       \n",
        "          - Suppose features is X and target labels is y.\n",
        "\n",
        "       3. Split the data\n",
        "\n",
        "          - test_size=0.25: 25% of the data goes to testing, 75% to training.\n",
        "          \n",
        "       4. Use the training data to fit the model, and test data to evaluate"
      ],
      "metadata": {
        "id": "nvf9MhKvfuMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of ques- 24 Spliting the data for model fitting (training and testing) in Python?\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8]]  # Features\n",
        "y = [0, 1, 0, 1]                      # Labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Training Data (X_train):\")\n",
        "print(X_train)\n",
        "print(\"\\nTesting Data (X_test):\")\n",
        "print(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLuuFpgxhCcB",
        "outputId": "e51caf09-68a9-4813-bb0d-faae312b3ee9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data (X_train):\n",
            "[[7, 8], [1, 2], [5, 6]]\n",
            "\n",
            "Testing Data (X_test):\n",
            "[[3, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "   - Data encoding is the process of converting categorical variables into a numerical format so that machine learning models can process them effectively.\n",
        "   \n",
        "   - Since ML algorithms work with numbers, encoding ensures that text-based or categorical data is represented in a way that models can interpret and learn from.\n",
        "\n",
        "   - Types of Categorical Data:\n",
        "   \n",
        "      1. Nominal data: No order or ranking (e.g., \"Red\", \"Blue\", \"Green\")\n",
        "      \n",
        "      2. Ordinal data: Has a clear order (e.g., \"Low\", \"Medium\", \"High\")\n",
        "\n",
        "   - Types of Data Encoding:\n",
        "\n",
        "     1. Label Encoding\n",
        "\n",
        "       - Assigns unique numerical labels to each category.\n",
        "       \n",
        "       - Works well for ordinal categories (where order matters).\n",
        "\n",
        "     2. One-Hot Encoding\n",
        "     \n",
        "       - Creates binary columns for each category.\n",
        "       \n",
        "       - Useful for nominal categorical variables (no order) and small categorial dataset.\n",
        "\n",
        "     3. Binary Encoding\n",
        "     \n",
        "       - Converts categories into binary values and encodes them efficiently.\n",
        "       \n",
        "       - Works well for large datasets with high cardinality (many unique values).\n",
        "\n",
        "     4. Frequency Encoding\n",
        "     \n",
        "       - Maps categories to the frequency of their occurrence in the dataset.\n",
        "       \n",
        "       - Helps when the number of unique categories is large.\n",
        "\n",
        "     5. Target Encoding (Mean Encoding)\n",
        "     \n",
        "       - Replaces categories with the average value of the target variable.\n",
        "      \n",
        "       - Useful in supervised learning tasks and features related to target variable."
      ],
      "metadata": {
        "id": "_2jYLxHUhZTf"
      }
    }
  ]
}